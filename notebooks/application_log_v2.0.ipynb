{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zD9bQ9TMz0oQ",
        "outputId": "763ed229-22db-4b15-c9ff-7964f28ea6a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting apachelogs\n",
            "  Downloading apachelogs-0.6.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: dask in /usr/local/lib/python3.11/dist-packages (2024.12.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.5.0)\n",
            "Requirement already satisfied: attrs>=17.1 in /usr/local/lib/python3.11/dist-packages (from apachelogs) (25.3.0)\n",
            "Collecting pydicti~=1.1 (from apachelogs)\n",
            "  Downloading pydicti-1.2.1-py2.py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.11/dist-packages (from dask) (8.2.1)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from dask) (3.1.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.11/dist-packages (from dask) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from dask) (24.2)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from dask) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from dask) (6.0.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from dask) (0.12.1)\n",
            "Requirement already satisfied: importlib_metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from dask) (8.7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (9.1.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.11/dist-packages (from seaborn) (3.10.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata>=4.13.0->dask) (3.21.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.11/dist-packages (from partd>=1.4.0->dask) (1.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading apachelogs-0.6.1-py3-none-any.whl (17 kB)\n",
            "Downloading pydicti-1.2.1-py2.py3-none-any.whl (9.0 kB)\n",
            "Installing collected packages: pydicti, apachelogs\n",
            "Successfully installed apachelogs-0.6.1 pydicti-1.2.1\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Downloading matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: matplotlib\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.0\n",
            "    Uninstalling matplotlib-3.10.0:\n",
            "      Successfully uninstalled matplotlib-3.10.0\n",
            "Successfully installed matplotlib-3.10.3\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install apachelogs dask pandas numpy scikit-learn tensorflow plotly seaborn joblib\n",
        "!pip install --upgrade matplotlib  # Ensure latest matplotlib version\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Import standard libraries\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import re\n",
        "import gzip\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Optional, Dict, List, Tuple\n",
        "\n",
        "# Import data processing libraries\n",
        "import dask.bag as db\n",
        "import apachelogs\n",
        "\n",
        "# Import ML libraries\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import joblib\n",
        "\n",
        "# Import visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "\n",
        "# Import additional libraries\n",
        "import urllib.parse\n",
        "import base64\n",
        "import html\n",
        "from itertools import combinations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_component_focused_logs_robust(line: str) -> Optional[Dict]:\n",
        "    \"\"\"Enhanced log parsing with better error handling and format flexibility\"\"\"\n",
        "    logger = logging.getLogger(__name__)\n",
        "\n",
        "    # Try apache log parser first\n",
        "    try:\n",
        "        formats = [\n",
        "            '%h %l %u %t \"%r\" %>s %b \"%{Referer}i\" \"%{User-Agent}i\"',  # Combined\n",
        "            '%h %l %u %t \"%r\" %>s %b',  # Common\n",
        "            '%h %l %u %t \"%r\" %>s'  # Basic\n",
        "        ]\n",
        "\n",
        "        for fmt in formats:\n",
        "            try:\n",
        "                parser = apachelogs.LogParser(fmt)\n",
        "                entry = parser.parse(line)\n",
        "\n",
        "                request_parts = entry.request_line.split(' ')\n",
        "                method = request_parts[0] if len(request_parts) > 0 else ''\n",
        "                url = request_parts[1] if len(request_parts) > 1 else ''\n",
        "                protocol = request_parts[2] if len(request_parts) > 2 else ''\n",
        "\n",
        "                return {\n",
        "                    'ip': entry.remote_host,\n",
        "                    'timestamp': entry.request_time,\n",
        "                    'method': method,\n",
        "                    'url': url,\n",
        "                    'protocol': protocol,\n",
        "                    'status': entry.final_status,\n",
        "                    'bytes': entry.bytes_sent,\n",
        "                    'referer': getattr(entry, 'headers_in', {}).get('Referer', ''),\n",
        "                    'user_agent': getattr(entry, 'headers_in', {}).get('User-Agent', '')\n",
        "                }\n",
        "            except:\n",
        "                continue\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Fallback to regex parsing\n",
        "    patterns = [\n",
        "        re.compile(r'(\\S+) \\S+ \\S+ $$ ([^ $$]+)\\] \"(\\S+) ([^\"]+) (\\S+)\" (\\d+) (\\d+|-) \"([^\"]*)\" \"([^\"]*)\"'),\n",
        "        re.compile(r'(\\S+) \\S+ \\S+ $$ ([^ $$]+)\\] \"(\\S+) ([^\"]+) (\\S+)\" (\\d+) (\\d+|-)'),\n",
        "        re.compile(r'(\\S+).*?$$ ([^ $$]+)\\].*?\"(\\S+) ([^\"]+) (\\S+)\" (\\d+)')\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        match = pattern.match(line.strip())\n",
        "        if match:\n",
        "            groups = match.groups()\n",
        "            while len(groups) < 9:\n",
        "                groups = groups + ('',)\n",
        "\n",
        "            return {\n",
        "                'ip': groups[0],\n",
        "                'timestamp': groups[1],\n",
        "                'method': groups[2],\n",
        "                'url': groups[3],\n",
        "                'protocol': groups[4],\n",
        "                'status': int(groups[5]) if groups[5].isdigit() else 0,\n",
        "                'bytes': int(groups[6]) if groups[6] and groups[6].isdigit() else 0,\n",
        "                'referer': groups[7] if len(groups) > 7 else '',\n",
        "                'user_agent': groups[8] if len(groups) > 8 else ''\n",
        "            }\n",
        "\n",
        "    logger.debug(f\"Failed to parse line: {line[:100]}...\")\n",
        "    return None\n",
        "\n",
        "def process_logs_distributed(file_paths: List[str], sample_size: Optional[int] = None) -> pd.DataFrame:\n",
        "    \"\"\"Process logs using Dask for scalability\"\"\"\n",
        "    logger = logging.getLogger(__name__)\n",
        "\n",
        "    config = {\n",
        "        'sample_size': sample_size,\n",
        "        'chunk_size': 10000,\n",
        "        'max_workers': 4\n",
        "    }\n",
        "\n",
        "    logger.info(f\"ðŸ” Processing {len(file_paths)} files with Dask...\")\n",
        "\n",
        "    def process_file(file_path):\n",
        "        records = []\n",
        "        line_count = 0\n",
        "        error_count = 0\n",
        "\n",
        "        try:\n",
        "            open_func = gzip.open if file_path.endswith('.gz') else open\n",
        "            mode = 'rt' if file_path.endswith('.gz') else 'r'\n",
        "\n",
        "            with open_func(file_path, mode, encoding='utf-8', errors='ignore') as f:\n",
        "                for line in f:\n",
        "                    if config['sample_size'] and line_count >= config['sample_size']:\n",
        "                        break\n",
        "\n",
        "                    parsed = parse_component_focused_logs_robust(line)\n",
        "                    if parsed:\n",
        "                        component_name, component_type, clean_url = extract_component_from_url_enhanced(parsed['url'])\n",
        "                        parsed.update({\n",
        "                            'clean_url': clean_url,\n",
        "                            'component_name': component_name,\n",
        "                            'component_type': component_type,\n",
        "                            'source_file': os.path.basename(file_path)\n",
        "                        })\n",
        "                        records.append(parsed)\n",
        "                    else:\n",
        "                        error_count += 1\n",
        "\n",
        "                    line_count += 1\n",
        "\n",
        "                    if line_count % 10000 == 0:\n",
        "                        logger.debug(f\"Processed {line_count} lines from {os.path.basename(file_path)}\")\n",
        "\n",
        "            logger.info(f\"âœ… Processed {os.path.basename(file_path)}: {len(records)} valid entries, {error_count} errors\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"âŒ Error processing {file_path}: {e}\")\n",
        "\n",
        "        return records\n",
        "\n",
        "    bag = db.from_sequence(file_paths)\n",
        "    results = bag.map(process_file).compute()\n",
        "    all_records = [record for file_records in results for record in file_records]\n",
        "\n",
        "    df = pd.DataFrame(all_records)\n",
        "    logger.info(f\"âœ… Processed {len(df)} total log entries from {len(file_paths)} files\")\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "Yvnq9AVT7W7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extract_component_from_url_enhanced(url: str) -> Tuple[str, str, str]:\n",
        "    \"\"\"Enhanced component extraction with better classification\"\"\"\n",
        "    logger = logging.getLogger(__name__)\n",
        "\n",
        "    try:\n",
        "        base_url = url.split('?')[0].split('#')[0]\n",
        "        path_parts = [part for part in base_url.split('/') if part]\n",
        "\n",
        "        if not path_parts:\n",
        "            return 'root', 'homepage', '/'\n",
        "\n",
        "        component_info = analyze_component_hierarchy(path_parts)\n",
        "\n",
        "        component_rules = {\n",
        "            'api': {'patterns': ['api', 'rest', 'graphql', 'v1', 'v2', 'v3', 'endpoint'], 'depth': 2},\n",
        "            'admin': {'patterns': ['admin', 'dashboard', 'manage', 'control', 'panel'], 'depth': 2},\n",
        "            'authentication': {'patterns': ['auth', 'login', 'logout', 'register', 'oauth', 'sso', 'signin', 'signup'], 'depth': 1},\n",
        "            'file_handler': {'patterns': ['upload', 'download', 'file', 'media', 'assets', 'static'], 'depth': 1},\n",
        "            'search': {'patterns': ['search', 'query', 'filter', 'find', 'lookup'], 'depth': 1},\n",
        "            'microservice': {'patterns': ['service', 'ms-', 'svc-'], 'depth': 1},\n",
        "            'database': {'patterns': ['db', 'mysql', 'postgres', 'mongo', 'redis'], 'depth': 1}\n",
        "        }\n",
        "\n",
        "        component_type = 'application'\n",
        "        component_name = path_parts[0] if path_parts else 'root'\n",
        "\n",
        "        for comp_type, rules in component_rules.items():\n",
        "            depth = rules.get('depth', 1)\n",
        "            check_parts = path_parts[:depth]\n",
        "\n",
        "            for part in check_parts:\n",
        "                if any(pattern in part.lower() for pattern in rules['patterns']):\n",
        "                    component_type = comp_type\n",
        "                    component_name = '/'.join(path_parts[:depth])\n",
        "                    break\n",
        "            if component_type != 'application':\n",
        "                break\n",
        "\n",
        "        if '.' in base_url:\n",
        "            extension = base_url.split('.')[-1].lower()\n",
        "            if extension in ['php', 'jsp', 'asp', 'py', 'rb', 'go']:\n",
        "                component_type = 'script'\n",
        "            elif extension in ['css', 'js', 'jpg', 'png', 'gif', 'svg', 'ico']:\n",
        "                component_type = 'static'\n",
        "                component_name = 'static_resources'\n",
        "\n",
        "        if len(path_parts) >= 3 and component_type == 'application':\n",
        "            if path_parts[0] in ['users', 'products', 'orders', 'payments', 'notifications']:\n",
        "                component_type = 'microservice'\n",
        "                component_name = f\"{path_parts[0]}/{path_parts[1]}\"\n",
        "\n",
        "        return component_name, component_type, base_url\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error extracting component from URL {url}: {e}\")\n",
        "        return 'unknown', 'unknown', url\n",
        "\n",
        "def analyze_component_hierarchy(path_parts: List[str]) -> Dict:\n",
        "    \"\"\"Analyze component hierarchy for better classification\"\"\"\n",
        "    hierarchy = {\n",
        "        'depth': len(path_parts),\n",
        "        'primary': path_parts[0] if path_parts else None,\n",
        "        'secondary': path_parts[1] if len(path_parts) > 1 else None,\n",
        "        'resource': path_parts[-1] if path_parts else None\n",
        "    }\n",
        "\n",
        "    if len(path_parts) >= 2:\n",
        "        id_pattern = re.compile(r'^(\\d+|[a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12})$')\n",
        "        hierarchy['has_id'] = any(id_pattern.match(part) for part in path_parts)\n",
        "        action_patterns = ['create', 'edit', 'delete', 'update', 'list', 'view', 'get', 'post']\n",
        "        hierarchy['has_action'] = any(action in path_parts[-1].lower() for action in action_patterns)\n",
        "\n",
        "    return hierarchy"
      ],
      "metadata": {
        "id": "EbBJWv1L70Ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def decode_payload(payload: str) -> str:\n",
        "    \"\"\"Decode potentially encoded payloads\"\"\"\n",
        "    decoded = payload\n",
        "    try:\n",
        "        decoded = urllib.parse.unquote(decoded)\n",
        "    except:\n",
        "        pass\n",
        "    try:\n",
        "        decoded = html.unescape(decoded)\n",
        "    except:\n",
        "        pass\n",
        "    if re.match(r'^[A-Za-z0-9+/]+={0,2}$', decoded) and len(decoded) % 4 == 0:\n",
        "        try:\n",
        "            decoded_bytes = base64.b64decode(decoded)\n",
        "            decoded = decoded_bytes.decode('utf-8', errors='ignore')\n",
        "        except:\n",
        "            pass\n",
        "    return decoded\n",
        "\n",
        "def detect_attack_patterns_enhanced(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Enhanced attack detection with confidence scoring\"\"\"\n",
        "    logger = logging.getLogger(__name__)\n",
        "\n",
        "    logger.info(\"ðŸŽ¯ Enhanced attack pattern detection...\")\n",
        "\n",
        "    df['decoded_url'] = df['url'].apply(decode_payload)\n",
        "\n",
        "    attack_signatures = {\n",
        "        'sql_injection': {\n",
        "            'patterns': [\n",
        "                (r'union.*select', 0.9),\n",
        "                (r'(order|group).*by.*\\d+', 0.7),\n",
        "                (r'(and|or)\\s*\\d+\\s*=\\s*\\d+', 0.8),\n",
        "                (r'(\\'|\")\\s*(and|or)\\s*(\\'|\")?\\s*\\d+\\s*=\\s*\\d+', 0.9),\n",
        "                (r'information_schema', 0.8),\n",
        "                (r'sysobjects|syscolumns', 0.9),\n",
        "                (r'exec\\s*$$ |execute\\s*\\(', 0.8),\n",
        "                (r'drop\\s+(table|database)', 0.95)\n",
        "            ],\n",
        "            'severity': 'high'\n",
        "        },\n",
        "        'xss': {\n",
        "            'patterns': [\n",
        "                (r'<script[^>]*>', 0.95),\n",
        "                (r'javascript:', 0.9),\n",
        "                (r'on(error|load|click|mouse\\w+)\\s*=', 0.85),\n",
        "                (r'(alert|prompt|confirm)\\s*\\(', 0.8),\n",
        "                (r'eval\\s*\\(', 0.9),\n",
        "                (r'document\\.(cookie|write)', 0.85),\n",
        "                (r'window\\.(location|open)', 0.7)\n",
        "            ],\n",
        "            'severity': 'high'\n",
        "        },\n",
        "        'directory_traversal': {\n",
        "            'patterns': [\n",
        "                (r'\\.\\./', 0.7),\n",
        "                (r'\\.\\.\\\\', 0.7),\n",
        "                (r'\\.{2,}[/\\\\]', 0.8),\n",
        "                (r'etc/passwd', 0.95),\n",
        "                (r'boot\\.ini', 0.95),\n",
        "                (r'windows/system32', 0.9),\n",
        "                (r'/proc/self', 0.9)\n",
        "            ],\n",
        "            'severity': 'medium'\n",
        "        },\n",
        "        'command_injection': {\n",
        "            'patterns': [\n",
        "                (r';.*?(cat|ls|id|whoami|pwd)', 0.9),\n",
        "                (r'\\|.*?(cat|ls|id|whoami)', 0.9),\n",
        "                (r'`.*?`', 0.7),\n",
        "                (r'\\$\\(.*? $$', 0.8),\n",
        "                (r'&&\\s*\\w+', 0.7),\n",
        "                (r'\\|\\|\\s*\\w+', 0.7)\n",
        "            ],\n",
        "            'severity': 'critical'\n",
        "        },\n",
        "        'file_inclusion': {\n",
        "            'patterns': [\n",
        "                (r'(file|include|require|page)\\s*=', 0.7),\n",
        "                (r'\\.php\\?', 0.6),\n",
        "                (r'(http|https|ftp)://', 0.7),\n",
        "                (r'php://input', 0.9),\n",
        "                (r'expect://', 0.9)\n",
        "            ],\n",
        "            'severity': 'high'\n",
        "        }\n",
        "    }\n",
        "\n",
        "    for attack_type, config in attack_signatures.items():\n",
        "        df[f'attack_{attack_type}_score'] = 0.0\n",
        "        df[f'attack_{attack_type}_matches'] = 0\n",
        "\n",
        "        for pattern, weight in config['patterns']:\n",
        "            matches = df['decoded_url'].str.contains(pattern, case=False, na=False, regex=True)\n",
        "            df[f'attack_{attack_type}_score'] += matches * weight\n",
        "            df[f'attack_{attack_type}_matches'] += matches\n",
        "\n",
        "        max_score = sum(weight for _, weight in config['patterns'])\n",
        "        df[f'attack_{attack_type}_score'] = df[f'attack_{attack_type}_score'] / max_score\n",
        "        df[f'attack_{attack_type}'] = df[f'attack_{attack_type}_score'] > 0.5\n",
        "\n",
        "    attack_score_columns = [col for col in df.columns if col.endswith('_score') and 'attack_' in col]\n",
        "    df['attack_confidence'] = df[attack_score_columns].max(axis=1)\n",
        "    df['is_attack'] = df['attack_confidence'] > 0.5\n",
        "\n",
        "    def get_attack_severity(row):\n",
        "        severities = []\n",
        "        for attack_type, config in attack_signatures.items():\n",
        "            if row[f'attack_{attack_type}']:\n",
        "                severities.append(config['severity'])\n",
        "        return 'critical' if 'critical' in severities else 'high' if 'high' in severities else 'medium' if 'medium' in severities else 'low'\n",
        "\n",
        "    df['attack_severity'] = df.apply(get_attack_severity, axis=1)\n",
        "\n",
        "    logger.info(f\"ðŸš¨ Attack detection complete: Total attacks: {df['is_attack'].sum()}, Critical: {(df['attack_severity'] == 'critical').sum()}\")\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "RctUdZvI78GB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def calculate_component_vulnerability_scores_configurable(df: pd.DataFrame, weights: Optional[Dict[str, float]] = None) -> pd.DataFrame:\n",
        "    \"\"\"Calculate vulnerability scores with configurable weights\"\"\"\n",
        "    logger = logging.getLogger(__name__)\n",
        "\n",
        "    logger.info(\"ðŸ“Š Calculating component vulnerability scores...\")\n",
        "\n",
        "    if weights is None:\n",
        "        weights = {\n",
        "            'attack_rate': 100.0, 'error_rate': 50.0, 'server_errors': 2.0, 'auth_errors': 3.0,\n",
        "            'attack_severity_critical': 5.0, 'attack_severity_high': 3.0, 'attack_severity_medium': 1.0,\n",
        "            'attack_confidence': 50.0, 'unique_attack_types': 10.0, 'failed_auth_rate': 75.0\n",
        "        }\n",
        "\n",
        "    component_stats = df.groupby(['component_name', 'component_type']).agg({\n",
        "        'ip': 'nunique', 'url': 'count', 'is_attack': ['sum', 'mean'], 'attack_confidence': 'mean',\n",
        "        'is_error': ['sum', 'mean'], 'is_server_error': ['sum', 'mean'], 'is_auth_error': ['sum', 'mean'],\n",
        "        'status': ['mean', 'std'], 'bytes': ['sum', 'mean', 'std'], 'timestamp': ['min', 'max']\n",
        "    }).reset_index()\n",
        "\n",
        "    severity_stats = df.groupby(['component_name', 'component_type', 'attack_severity']).size().unstack(fill_value=0).reset_index()\n",
        "    component_stats = component_stats.merge(severity_stats, on=['component_name', 'component_type'], how='left')\n",
        "\n",
        "    attack_types = [col for col in df.columns if col.startswith('attack_') and col.endswith('_score')]\n",
        "    unique_attacks = df.groupby(['component_name', 'component_type'])[attack_types].apply(\n",
        "        lambda x: (x > 0).any().sum()\n",
        "    ).reset_index(name='unique_attack_types')\n",
        "\n",
        "    component_stats = component_stats.merge(unique_attacks, on=['component_name', 'component_type'], how='left')\n",
        "\n",
        "    component_stats.columns = [col[0] if col[1] == '' else f\"{col[0]}_{col[1]}\" for col in component_stats.columns]\n",
        "\n",
        "    rename_dict = {\n",
        "        'ip_nunique': 'unique_ips', 'url_count': 'total_requests', 'is_attack_sum': 'attack_count',\n",
        "        'is_attack_mean': 'attack_rate', 'attack_confidence_mean': 'avg_attack_confidence',\n",
        "        'is_error_sum': 'error_count', 'is_error_mean': 'error_rate', 'is_server_error_sum': 'server_errors',\n",
        "        'is_server_error_mean': 'server_error_rate', 'is_auth_error_sum': 'auth_errors',\n",
        "        'is_auth_error_mean': 'auth_error_rate', 'status_mean': 'avg_status', 'status_std': 'status_std',\n",
        "        'bytes_sum': 'total_bytes', 'bytes_mean': 'avg_bytes', 'bytes_std': 'bytes_std',\n",
        "        'timestamp_min': 'first_access', 'timestamp_max': 'last_access'\n",
        "    }\n",
        "\n",
        "    component_stats.rename(columns=rename_dict, inplace=True)\n",
        "\n",
        "    for severity in ['critical', 'high', 'medium', 'low']:\n",
        "        if severity not in component_stats.columns:\n",
        "            component_stats[severity] = 0\n",
        "\n",
        "    component_stats['vulnerability_score'] = (\n",
        "        weights.get('attack_rate', 100) * component_stats['attack_rate'] +\n",
        "        weights.get('error_rate', 50) * component_stats['error_rate'] +\n",
        "        weights.get('server_errors', 2) * component_stats['server_errors'] +\n",
        "        weights.get('auth_errors', 3) * component_stats['auth_errors'] +\n",
        "        weights.get('attack_severity_critical', 5) * component_stats.get('critical', 0) +\n",
        "        weights.get('attack_severity_high', 3) * component_stats.get('high', 0) +\n",
        "        weights.get('attack_severity_medium', 1) * component_stats.get('medium', 0) +\n",
        "        weights.get('attack_confidence', 50) * component_stats['avg_attack_confidence'] +\n",
        "        weights.get('unique_attack_types', 10) * component_stats['unique_attack_types'] +\n",
        "        weights.get('failed_auth_rate', 75) * component_stats['auth_error_rate']\n",
        "    )\n",
        "\n",
        "    def sigmoid_scale(x, midpoint=100, steepness=0.01):\n",
        "        return 1 / (1 + np.exp(-steepness * (x - midpoint)))\n",
        "\n",
        "    component_stats['traffic_factor'] = component_stats['total_requests'].apply(\n",
        "        lambda x: 0.5 + 0.5 * sigmoid_scale(x, midpoint=100, steepness=0.01)\n",
        "    )\n",
        "\n",
        "    component_stats['risk_score'] = component_stats['vulnerability_score'] * component_stats['traffic_factor']\n",
        "\n",
        "    critical_components = ['authentication', 'admin', 'api']\n",
        "    is_critical = component_stats['component_type'].isin(critical_components)\n",
        "    component_stats.loc[is_critical, 'risk_score'] = component_stats.loc[is_critical, 'risk_score'].clip(lower=50)\n",
        "\n",
        "    component_stats['exposure_score'] = component_stats['unique_ips'] / (component_stats['total_requests'] + 1)\n",
        "    component_stats['activity_duration'] = (\n",
        "        component_stats['last_access'] - component_stats['first_access']\n",
        "    ).dt.total_seconds() / 3600\n",
        "    component_stats['requests_per_hour'] = component_stats['total_requests'] / (component_stats['activity_duration'] + 0.01)\n",
        "\n",
        "    component_stats = component_stats.sort_values('risk_score', ascending=False)\n",
        "\n",
        "    logger.info(f\"ðŸ“ˆ Vulnerability scoring completed for {len(component_stats)} components\")\n",
        "\n",
        "    return component_stats"
      ],
      "metadata": {
        "id": "4HmWrFHc8CWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_ensemble_vulnerability_model(component_stats: pd.DataFrame, use_synthetic_labels: bool = False):\n",
        "    \"\"\"Train ensemble model with multiple algorithms\"\"\"\n",
        "    logger = logging.getLogger(__name__)\n",
        "\n",
        "    logger.info(\"ðŸ¤– Training ensemble vulnerability prediction model...\")\n",
        "\n",
        "    feature_columns = [\n",
        "        'unique_ips', 'total_requests', 'attack_count', 'attack_rate',\n",
        "        'error_count', 'error_rate', 'server_errors', 'auth_errors',\n",
        "        'avg_status', 'status_std', 'exposure_score', 'requests_per_hour',\n",
        "        'avg_attack_confidence', 'unique_attack_types', 'server_error_rate',\n",
        "        'auth_error_rate', 'traffic_factor'\n",
        "    ]\n",
        "\n",
        "    for severity in ['critical', 'high', 'medium']:\n",
        "        if severity in component_stats.columns:\n",
        "            feature_columns.append(severity)\n",
        "\n",
        "    X = component_stats[feature_columns].fillna(0)\n",
        "\n",
        "    if use_synthetic_labels:\n",
        "        y = create_synthetic_vulnerability_labels(component_stats)\n",
        "    else:\n",
        "        vulnerability_conditions = (\n",
        "            (component_stats['attack_rate'] > 0.1) |\n",
        "            (component_stats['server_error_rate'] > 0.05) |\n",
        "            (component_stats['auth_error_rate'] > 0.1) |\n",
        "            (component_stats.get('critical', 0) > 0) |\n",
        "            (component_stats['risk_score'] > component_stats['risk_score'].quantile(0.8))\n",
        "        )\n",
        "        y = vulnerability_conditions.astype(int)\n",
        "\n",
        "    logger.info(f\"ðŸ· Label distribution - Vulnerable: {y.sum()}, Safe: {len(y) - y.sum()}\")\n",
        "\n",
        "    le_component_type = LabelEncoder()\n",
        "    component_type_encoded = le_component_type.fit_transform(component_stats['component_type'])\n",
        "\n",
        "    X_with_type = np.column_stack([X.values, component_type_encoded])\n",
        "    feature_columns_extended = feature_columns + ['component_type_encoded']\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X_with_type)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "    logger.info(\"ðŸŒ² Training Random Forest...\")\n",
        "    rf_params = {'n_estimators': [100, 200], 'max_depth': [10, 20, None], 'min_samples_split': [2, 5], 'class_weight': ['balanced']}\n",
        "    rf_model = GridSearchCV(RandomForestClassifier(random_state=42), rf_params, cv=5, scoring='f1', n_jobs=-1)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "\n",
        "    logger.info(\"ðŸš€ Training Gradient Boosting...\")\n",
        "    gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
        "    gb_model.fit(X_train, y_train)\n",
        "\n",
        "    logger.info(\"ðŸ§  Training Neural Network...\")\n",
        "    mlp_model = MLPClassifier(hidden_layer_sizes=(100, 50, 25), activation='relu', solver='adam', alpha=0.001, max_iter=500, random_state=42)\n",
        "    mlp_model.fit(X_train, y_train)\n",
        "\n",
        "    logger.info(\"ðŸ” Training One-Class SVM...\")\n",
        "    X_normal = X_scaled[y == 0]\n",
        "    oc_svm = None\n",
        "    if len(X_normal) > 10:\n",
        "        oc_svm = OneClassSVM(kernel='rbf', gamma='auto', nu=0.1)\n",
        "        oc_svm.fit(X_normal)\n",
        "    else:\n",
        "        logger.warning(\"âš  Insufficient normal samples for One-Class SVM\")\n",
        "\n",
        "    logger.info(\"ðŸŽ­ Creating Ensemble Model...\")\n",
        "    ensemble_model = VotingClassifier(estimators=[('rf', rf_model.best_estimator_), ('gb', gb_model), ('mlp', mlp_model)], voting='soft')\n",
        "    ensemble_model.fit(X_train, y_train)\n",
        "\n",
        "    models = {'Random Forest': rf_model.best_estimator_, 'Gradient Boosting': gb_model, 'Neural Network': mlp_model, 'Ensemble': ensemble_model}\n",
        "    results = {}\n",
        "\n",
        "    for name, model in models.items():\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "        report = classification_report(y_test, y_pred, output_dict=True)\n",
        "        auc_score = roc_auc_score(y_test, y_pred_proba)\n",
        "        results[name] = {'precision': report['1']['precision'], 'recall': report['1']['recall'], 'f1': report['1']['f1-score'], 'auc': auc_score}\n",
        "        logger.info(f\"ðŸ“Š {name} Results: Precision: {report['1']['precision']:.3f}, Recall: {report['1']['recall']:.3f}, F1: {report['1']['f1-score']:.3f}, AUC: {auc_score:.3f}\")\n",
        "\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': feature_columns_extended,\n",
        "        'rf_importance': rf_model.best_estimator_.feature_importances_,\n",
        "        'gb_importance': gb_model.feature_importances_\n",
        "    })\n",
        "    feature_importance['avg_importance'] = (feature_importance['rf_importance'] + feature_importance['gb_importance']) / 2\n",
        "    feature_importance = feature_importance.sort_values('avg_importance', ascending=False)\n",
        "\n",
        "    logger.info(\"ðŸŽ¯ Top 10 Most Important Features:\")\n",
        "    for _, row in feature_importance.head(10).iterrows():\n",
        "        logger.info(f\"   {row['feature']}: {row['avg_importance']:.4f}\")\n",
        "\n",
        "    all_predictions = ensemble_model.predict_proba(X_scaled)[:, 1]\n",
        "    component_stats['vulnerability_probability'] = all_predictions\n",
        "\n",
        "    if oc_svm is not None:\n",
        "        anomaly_scores = oc_svm.decision_function(X_scaled)\n",
        "        component_stats['anomaly_score'] = -anomaly_scores\n",
        "\n",
        "    model_package = {\n",
        "        'ensemble_model': ensemble_model, 'rf_model': rf_model.best_estimator_, 'gb_model': gb_model,\n",
        "        'mlp_model': mlp_model, 'oc_svm': oc_svm, 'scaler': scaler, 'label_encoder': le_component_type,\n",
        "        'feature_columns': feature_columns_extended, 'feature_importance': feature_importance, 'model_results': results\n",
        "    }\n",
        "\n",
        "    joblib.dump(model_package, '/content/drive/My Drive/ensemble_vulnerability_model.pkl')\n",
        "    logger.info(\"ðŸ’¾ Ensemble model saved to ensemble_vulnerability_model.pkl\")\n",
        "\n",
        "    return model_package, component_stats\n",
        "\n",
        "def create_synthetic_vulnerability_labels(component_stats: pd.DataFrame) -> np.ndarray:\n",
        "    \"\"\"Create synthetic labels based on OWASP patterns\"\"\"\n",
        "    vulnerability_patterns = [\n",
        "        (component_stats['component_type'] == 'api') & (component_stats['attack_rate'] > 0.05) & (component_stats.get('sql_injection', 0) > 0),\n",
        "        (component_stats['component_type'] == 'authentication') & (component_stats['auth_error_rate'] > 0.1),\n",
        "        (component_stats['component_type'] == 'file_handler') & (component_stats['error_rate'] > 0.05),\n",
        "        (component_stats['component_type'] == 'admin') & (component_stats['unique_ips'] > 10),\n",
        "        component_stats['attack_rate'] > 0.2,\n",
        "        component_stats['server_error_rate'] > 0.1,\n",
        "        (component_stats.get('critical', 0) > 0) | (component_stats.get('high', 0) > 5)\n",
        "    ]\n",
        "\n",
        "    y = np.zeros(len(component_stats))\n",
        "    for pattern in vulnerability_patterns:\n",
        "        y = y | pattern.values\n",
        "\n",
        "    return y.astype(int)"
      ],
      "metadata": {
        "id": "kvIwh5Nl8QlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def analyze_attack_component_correlation_enhanced(df: pd.DataFrame, component_stats: pd.DataFrame) -> Tuple[pd.DataFrame, Dict, pd.DataFrame]:\n",
        "    \"\"\"Enhanced correlation analysis with application log integration\"\"\"\n",
        "    logger = logging.getLogger(__name__)\n",
        "\n",
        "    logger.info(\"ðŸ”— Enhanced attack-component correlation analysis...\")\n",
        "\n",
        "    attack_columns = [col for col in df.columns if col.startswith('attack_') and not col.endswith('_score')]\n",
        "    correlation_data = []\n",
        "\n",
        "    for attack_type in attack_columns:\n",
        "        attack_name = attack_type.replace('attack_', '')\n",
        "        attack_df = df[df[attack_type] == True]\n",
        "\n",
        "        if len(attack_df) > 0:\n",
        "            component_attack_stats = attack_df.groupby(['component_name', 'component_type']).agg({\n",
        "                'url': 'count', 'is_error': 'sum', 'is_server_error': 'sum', 'status': ['mean', 'std'],\n",
        "                'attack_confidence': 'mean', 'ip': 'nunique'\n",
        "            }).reset_index()\n",
        "\n",
        "            component_attack_stats.columns = [\n",
        "                'component_name', 'component_type', 'attack_attempts', 'error_count', 'server_error_count',\n",
        "                'avg_status', 'status_std', 'avg_confidence', 'unique_attackers'\n",
        "            ]\n",
        "\n",
        "            component_attack_stats['attack_success_rate'] = component_attack_stats['error_count'] / component_attack_stats['attack_attempts']\n",
        "            component_attack_stats['attack_type'] = attack_name\n",
        "            correlation_data.append(component_attack_stats)\n",
        "\n",
        "    if correlation_data:\n",
        "        full_correlation_df = pd.concat(correlation_data, ignore_index=True)\n",
        "\n",
        "        pivot_metrics = ['attack_attempts', 'attack_success_rate', 'avg_confidence']\n",
        "        pivot_tables = {}\n",
        "\n",
        "        for metric in pivot_metrics:\n",
        "            pivot_tables[metric] = full_correlation_df.pivot_table(\n",
        "                index=['component_name', 'component_type'], columns='attack_type', values=metric, fill_value=0, aggfunc='mean'\n",
        "            ).reset_index()\n",
        "\n",
        "        correlation_matrix = full_correlation_df.pivot_table(\n",
        "            index='component_name', columns='attack_type', values='attack_attempts', fill_value=0\n",
        "        )\n",
        "        correlation_matrix_normalized = correlation_matrix.div(correlation_matrix.sum(axis=1), axis=0).fillna(0)\n",
        "\n",
        "        attack_patterns = identify_attack_patterns(full_correlation_df)\n",
        "        vulnerability_profiles = create_vulnerability_profiles(full_correlation_df, component_stats)\n",
        "\n",
        "        logger.info(\"ðŸŽ¯ Attack-Component Correlation Insights:\")\n",
        "        for attack_type in attack_columns:\n",
        "            attack_name = attack_type.replace('attack_', '')\n",
        "            attack_data = full_correlation_df[full_correlation_df['attack_type'] == attack_name]\n",
        "            if len(attack_data) > 0:\n",
        "                top_target = attack_data.nlargest(1, 'attack_attempts').iloc[0]\n",
        "                logger.info(f\"   {attack_name.upper()} -> {top_target['component_name']} ({top_target['attack_attempts']} attempts, {top_target['attack_success_rate']:.2%} success rate)\")\n",
        "\n",
        "        return full_correlation_df, pivot_tables, vulnerability_profiles\n",
        "    return pd.DataFrame(), {}, pd.DataFrame()\n",
        "\n",
        "def identify_attack_patterns(correlation_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Identify common attack patterns and sequences\"\"\"\n",
        "    attack_combinations = correlation_df.groupby('component_name')['attack_type'].apply(lambda x: list(x.unique())).reset_index()\n",
        "\n",
        "    attack_pairs = []\n",
        "    for attacks in attack_combinations['attack_type']:\n",
        "        if len(attacks) > 1:\n",
        "            for pair in combinations(attacks, 2):\n",
        "                attack_pairs.append(sorted(pair))\n",
        "\n",
        "    pair_counts = pd.Series(['-'.join(pair) for pair in attack_pairs]).value_counts()\n",
        "\n",
        "    return pd.DataFrame({'attack_pattern': pair_counts.index, 'frequency': pair_counts.values})\n",
        "\n",
        "def create_vulnerability_profiles(correlation_df: pd.DataFrame, component_stats: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Create comprehensive vulnerability profiles for components\"\"\"\n",
        "    component_profiles = correlation_df.groupby(['component_name', 'component_type']).agg({\n",
        "        'attack_attempts': 'sum', 'error_count': 'sum', 'server_error_count': 'sum',\n",
        "        'attack_success_rate': 'mean', 'avg_confidence': 'mean', 'unique_attackers': 'max', 'attack_type': 'nunique'\n",
        "    }).reset_index()\n",
        "\n",
        "    component_profiles.columns = [\n",
        "        'component_name', 'component_type', 'total_attacks', 'total_errors', 'total_server_errors',\n",
        "        'avg_success_rate', 'avg_attack_confidence', 'max_unique_attackers', 'attack_diversity'\n",
        "    ]\n",
        "\n",
        "    profiles = component_profiles.merge(\n",
        "        component_stats[['component_name', 'risk_score', 'vulnerability_probability']],\n",
        "        on='component_name', how='left'\n",
        "    )\n",
        "\n",
        "    profiles['profile_score'] = (\n",
        "        profiles['total_attacks'] * 0.3 + profiles['avg_success_rate'] * 100 * 0.3 +\n",
        "        profiles['attack_diversity'] * 10 * 0.2 + profiles['max_unique_attackers'] * 0.2\n",
        "    )\n",
        "\n",
        "    def categorize_profile(row):\n",
        "        if row['attack_diversity'] >= 3 and row['avg_success_rate'] > 0.5:\n",
        "            return 'critical_multi_vector'\n",
        "        elif row['total_attacks'] > 100 and row['avg_success_rate'] > 0.3:\n",
        "            return 'high_volume_vulnerable'\n",
        "        elif row['attack_diversity'] >= 2:\n",
        "            return 'multi_vector_target'\n",
        "        elif row['avg_success_rate'] > 0.7:\n",
        "            return 'highly_vulnerable'\n",
        "        elif row['total_attacks'] > 50:\n",
        "            return 'frequent_target'\n",
        "        return 'low_risk'\n",
        "\n",
        "    profiles['vulnerability_profile'] = profiles.apply(categorize_profile, axis=1)\n",
        "\n",
        "    return profiles.sort_values('profile_score', ascending=False)"
      ],
      "metadata": {
        "id": "b2s_MUE88UlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def parse_application_logs(app_log_files: List[str]) -> pd.DataFrame:\n",
        "    \"\"\"Parse application logs to correlate with access logs\"\"\"\n",
        "    logger = logging.getLogger(__name__)\n",
        "\n",
        "    logger.info(\"ðŸ“± Parsing application logs for correlation...\")\n",
        "\n",
        "    app_records = []\n",
        "    patterns = {\n",
        "        'error': re.compile(r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}).*?(ERROR|FATAL|CRITICAL).*?(\\w+Exception|\\w+Error)?.*?(.+)'),\n",
        "        'warning': re.compile(r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}).*?(WARN|WARNING).*?(.+)'),\n",
        "        'stack_trace': re.compile(r'at\\s+[\\w\\.$]+$$ [\\w\\.]+:\\d+ $$'),\n",
        "        'sql_error': re.compile(r'(SQL|SQLException|Database).*?(error|exception)', re.IGNORECASE),\n",
        "        'auth_failure': re.compile(r'(authentication|authorization|login|access).*?(failed|denied|invalid)', re.IGNORECASE)\n",
        "    }\n",
        "\n",
        "    for file_path in app_log_files:\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                current_error = None\n",
        "                for line in f:\n",
        "                    for pattern_name, pattern in patterns.items():\n",
        "                        match = pattern.search(line)\n",
        "                        if match:\n",
        "                            if pattern_name in ['error', 'warning']:\n",
        "                                if current_error:\n",
        "                                    app_records.append(current_error)\n",
        "                                current_error = {\n",
        "                                    'timestamp': match.group(1),\n",
        "                                    'level': match.group(2),\n",
        "                                    'exception': match.group(3) if len(match.groups()) > 2 else None,\n",
        "                                    'message': match.group(4) if len(match.groups()) > 3 else match.group(3),\n",
        "                                    'type': pattern_name,\n",
        "                                    'stack_trace': []\n",
        "                                }\n",
        "                            elif pattern_name == 'stack_trace' and current_error:\n",
        "                                current_error['stack_trace'].append(line.strip())\n",
        "                            elif pattern_name in ['sql_error', 'auth_failure']:\n",
        "                                if current_error:\n",
        "                                    current_error['type'] = pattern_name\n",
        "                if current_error:\n",
        "                    app_records.append(current_error)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error parsing application log {file_path}: {e}\")\n",
        "\n",
        "    app_logs_df = pd.DataFrame(app_records)\n",
        "\n",
        "    if not app_logs_df.empty:\n",
        "        app_logs_df['timestamp'] = pd.to_datetime(app_logs_df['timestamp'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
        "        app_logs_df['component'] = app_logs_df['stack_trace'].apply(extract_component_from_stacktrace)\n",
        "        logger.info(f\"âœ… Parsed {len(app_logs_df)} application log entries\")\n",
        "\n",
        "    return app_logs_df\n",
        "\n",
        "def extract_component_from_stacktrace(stack_trace: List[str]) -> str:\n",
        "    \"\"\"Extract component name from stack trace\"\"\"\n",
        "    if not stack_trace:\n",
        "        return 'unknown'\n",
        "\n",
        "    for line in stack_trace[:5]:\n",
        "        match = re.search(r'at\\s+([\\w\\.]+)\\.([\\w]+)\\(', line)\n",
        "        if match:\n",
        "            package = match.group(1)\n",
        "            parts = package.split('.')\n",
        "            if len(parts) > 2:\n",
        "                return parts[-2] if parts[-2] not in ['util', 'common', 'core'] else parts[-3]\n",
        "    return 'unknown'\n",
        "\n",
        "def correlate_access_and_app_logs(access_logs: pd.DataFrame, app_logs: pd.DataFrame, time_window: int = 5) -> pd.DataFrame:\n",
        "    \"\"\"Correlate access logs with application logs within time windows\"\"\"\n",
        "    logger = logging.getLogger(__name__)\n",
        "\n",
        "    logger.info(\"ðŸ”— Correlating access and application logs...\")\n",
        "\n",
        "    correlations = []\n",
        "    access_logs['timestamp'] = pd.to_datetime(access_logs['timestamp'])\n",
        "\n",
        "    for _, app_error in app_logs.iterrows():\n",
        "        error_time = app_error['timestamp']\n",
        "        time_start = error_time - pd.Timedelta(seconds=time_window)\n",
        "        time_end = error_time + pd.Timedelta(seconds=time_window)\n",
        "\n",
        "        related_access = access_logs[(access_logs['timestamp'] >= time_start) & (access_logs['timestamp'] <= time_end)]\n",
        "\n",
        "        if not related_access.empty:\n",
        "            for _, access in related_access.iterrows():\n",
        "                correlation = {\n",
        "                    'app_error_time': error_time,\n",
        "                    'access_time': access['timestamp'],\n",
        "                    'time_diff': abs((error_time - access['timestamp']).total_seconds()),\n",
        "                    'component': access['component_name'],\n",
        "                    'url': access['url'],\n",
        "                    'status': access['status'],\n",
        "                    'is_attack': access['is_attack'],\n",
        "                    'error_type': app_error['type'],\n",
        "                    'error_level': app_error['level'],\n",
        "                    'exception': app_error.get('exception', ''),\n",
        "                    'error_message': app_error['message'][:200]\n",
        "                }\n",
        "                correlations.append(correlation)\n",
        "\n",
        "    correlation_df = pd.DataFrame(correlations)\n",
        "\n",
        "    if not correlation_df.empty:\n",
        "        correlation_df = correlation_df.sort_values('time_diff').drop_duplicates(subset=['app_error_time', 'component'], keep='first')\n",
        "        logger.info(f\"âœ… Found {len(correlation_df)} access-application log correlations\")\n",
        "\n",
        "        attack_induced_errors = correlation_df[correlation_df['is_attack'] == True]\n",
        "        logger.info(f\"ðŸš¨ {len(attack_induced_errors)} errors potentially caused by attacks\")\n",
        "\n",
        "        component_errors = correlation_df.groupby('component').agg({'error_type': 'count', 'is_attack': 'sum'}).reset_index()\n",
        "        component_errors.columns = ['component', 'error_count', 'attack_induced_errors']\n",
        "\n",
        "        return correlation_df, component_errors\n",
        "    logger.warning(\"âš  No correlations found between access and application logs\")\n",
        "    return pd.DataFrame(), pd.DataFrame()"
      ],
      "metadata": {
        "id": "1bA9r1Zw8X7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_comprehensive_visualizations(component_df: pd.DataFrame, component_stats: pd.DataFrame, correlation_df: pd.DataFrame, model_results: Dict):\n",
        "    \"\"\"Generate comprehensive research visualizations\"\"\"\n",
        "    logger = logging.getLogger(__name__)\n",
        "\n",
        "    logger.info(\"ðŸ“Š Generating comprehensive visualizations...\")\n",
        "\n",
        "    plt.style.use('seaborn-v0_8-darkgrid')\n",
        "    sns.set_palette(\"husl\")\n",
        "\n",
        "    fig = plt.figure(figsize=(20, 15))\n",
        "\n",
        "    ax1 = plt.subplot(3, 3, 1)\n",
        "    component_stats['risk_score'].hist(bins=30, ax=ax1)\n",
        "    ax1.set_title('Component Risk Score Distribution', fontsize=14)\n",
        "    ax1.set_xlabel('Risk Score')\n",
        "    ax1.set_ylabel('Number of Components')\n",
        "\n",
        "    ax2 = plt.subplot(3, 3, 2)\n",
        "    top_10 = component_stats.head(10)\n",
        "    ax2.barh(range(len(top_10)), top_10['risk_score'])\n",
        "    ax2.set_yticks(range(len(top_10)))\n",
        "    ax2.set_yticklabels(top_10['component_name'])\n",
        "    ax2.set_title('Top 10 Vulnerable Components', fontsize=14)\n",
        "    ax2.set_xlabel('Risk Score')\n",
        "\n",
        "    ax3 = plt.subplot(3, 3, 3)\n",
        "    attack_columns = [col for col in component_df.columns if col.startswith('attack_') and not col.endswith('_score')]\n",
        "    attack_counts = component_df[attack_columns].sum().sort_values(ascending=False)\n",
        "    ax3.bar(range(len(attack_counts)), attack_counts.values)\n",
        "    ax3.set_xticks(range(len(attack_counts)))\n",
        "    ax3.set_xticklabels([col.replace('attack_', '') for col in attack_counts.index], rotation=45)\n",
        "    ax3.set_title('Attack Type Distribution', fontsize=14)\n",
        "    ax3.set_ylabel('Number of Attacks')\n",
        "\n",
        "    ax4 = plt.subplot(3, 3, 4)\n",
        "    pivot_data = component_stats.pivot_table(index='component_type', values=['attack_rate', 'error_rate', 'server_error_rate'], aggfunc='mean')\n",
        "    sns.heatmap(pivot_data, annot=True, fmt='.3f', cmap='YlOrRd', ax=ax4)\n",
        "    ax4.set_title('Component Type Vulnerability Rates', fontsize=14)\n",
        "\n",
        "    ax5 = plt.subplot(3, 3, 5)\n",
        "    if not correlation_df.empty:\n",
        "        success_rates = correlation_df.groupby('component_type')['attack_success_rate'].mean().sort_values(ascending=False)\n",
        "        ax5.bar(range(len(success_rates)), success_rates.values)\n",
        "        ax5.set_xticks(range(len(success_rates)))\n",
        "        ax5.set_xticklabels(success_rates.index, rotation=45)\n",
        "        ax5.set_title('Attack Success Rate by Component Type', fontsize=14)\n",
        "        ax5.set_ylabel('Success Rate')\n",
        "\n",
        "    ax6 = plt.subplot(3, 3, 6)\n",
        "    if model_results:\n",
        "        metrics = ['precision', 'recall', 'f1', 'auc']\n",
        "        model_names = list(model_results.keys())\n",
        "        x = np.arange(len(metrics))\n",
        "        width = 0.2\n",
        "        for i, model in enumerate(model_names):\n",
        "            values = [model_results[model][metric] for metric in metrics]\n",
        "            ax6.bar(x + i * width, values, width, label=model)\n",
        "        ax6.set_xticks(x + width * 1.5)\n",
        "        ax6.set_xticklabels(metrics)\n",
        "        ax6.set_title('Model Performance Comparison', fontsize=14)\n",
        "        ax6.set_ylabel('Score')\n",
        "        ax6.legend()\n",
        "\n",
        "    ax7 = plt.subplot(3, 3, 7)\n",
        "    if 'timestamp' in component_df.columns:\n",
        "        component_df['hour'] = pd.to_datetime(component_df['timestamp']).dt.hour\n",
        "        hourly_attacks = component_df.groupby('hour').agg({'is_attack': 'sum'})\n",
        "        ax7.plot(hourly_attacks.index, hourly_attacks.values, marker='o')\n",
        "        ax7.set_title('Hourly Attack Distribution', fontsize=14)\n",
        "        ax7.set_xlabel('Hour of Day')\n",
        "        ax7.set_ylabel('Number of Attacks')\n",
        "        ax7.set_xticks(range(0, 24, 4))\n",
        "\n",
        "    ax8 = plt.subplot(3, 3, 8)\n",
        "    if 'vulnerability_probability' in component_stats.columns:\n",
        "        component_stats['vulnerability_probability'].hist(bins=30, ax=ax8)\n",
        "        ax8.axvline(0.5, color='red', linestyle='--', label='Threshold')\n",
        "        ax8.set_title('ML Vulnerability Probability Distribution', fontsize=14)\n",
        "        ax8.set_xlabel('Probability')\n",
        "        ax8.set_ylabel('Number of Components')\n",
        "        ax8.legend()\n",
        "\n",
        "    ax9 = plt.subplot(3, 3, 9)\n",
        "    if 'feature_importance' in model_results:\n",
        "        top_features = model_results['feature_importance'].head(10)\n",
        "        ax9.barh(range(len(top_features)), top_features['avg_importance'])\n",
        "        ax9.set_yticks(range(len(top_features)))\n",
        "        ax9.set_yticklabels(top_features['feature'])\n",
        "        ax9.set_title('Top 10 Feature Importances', fontsize=14)\n",
        "        ax9.set_xlabel('Importance')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/drive/My Drive/vulnerability_analysis_dashboard.png', dpi=150, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "\n",
        "    create_interactive_visualizations(component_stats, correlation_df)\n",
        "\n",
        "    logger.info(\"âœ… Visualizations generated and saved\")\n",
        "\n",
        "def create_interactive_visualizations(component_stats: pd.DataFrame, correlation_df: pd.DataFrame):\n",
        "    \"\"\"Create interactive visualizations using Plotly\"\"\"\n",
        "    logger = logging.getLogger(__name__)\n",
        "\n",
        "    fig1 = go.Figure(data=[go.Scatter3d(\n",
        "        x=component_stats['attack_rate'], y=component_stats['error_rate'], z=component_stats['risk_score'],\n",
        "        mode='markers+text', text=component_stats['component_name'], textposition=\"top center\",\n",
        "        marker=dict(size=component_stats['total_requests'] / 100, color=component_stats['vulnerability_probability'], colorscale='Viridis', showscale=True, colorbar=dict(title=\"Vulnerability Probability\"))\n",
        "    )])\n",
        "\n",
        "    fig1.update_layout(\n",
        "        title='3D Component Vulnerability Score',\n",
        "        scene=dict(xaxis_title='Attack Rate', yaxis_title='Error Rate', zaxis_title='Risk Score')\n",
        "    )\n",
        "    fig1.write_html('/content/drive/My Drive/component_vulnerability_3d.html')\n",
        "\n",
        "    if not correlation_df.empty:\n",
        "        attack_flow = correlation_df.groupby(['attack_type', 'component_type']).size().reset_index(name='attack_count')\n",
        "        attack_types = attack_flow['attack_type'].unique()\n",
        "        component_types = attack_flow['component_type'].unique()\n",
        "        labels = list(attack_types) + list(component_types)\n",
        "        source = [list(attack_types).index(x) for x in attack_flow['attack_type']]\n",
        "        target = [len(attack_types) + list(component_types).index(x) for x in attack_flow['component_type']]\n",
        "        value = attack_flow['attack_count'].tolist()\n",
        "\n",
        "        fig2 = go.Figure(data=[go.Sankey(\n",
        "            node=dict(pad=15, thickness=20, line=dict(color=\"black\", width=0.5), label=labels, color=\"blue\"),\n",
        "            link=dict(source=source, target=target, value=value)\n",
        "        )])\n",
        "\n",
        "        fig2.update_layout(title=\"Attack Type to Component Type Flow\", font_size=12)\n",
        "        fig2.write_html('/content/drive/My Drive/attack_flow_sankey.html')\n",
        "\n",
        "    logger.info(\"âœ… Interactive visualizations created\")"
      ],
      "metadata": {
        "id": "COCixYG28awl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class VulnerabilityDetectionPipeline:\n",
        "    \"\"\"Production-ready vulnerability detection pipeline\"\"\"\n",
        "\n",
        "    def __init__(self, config: Dict):\n",
        "        self.config = config\n",
        "        self.logger = self._setup_logging()\n",
        "        self.models = {}\n",
        "        self.scalers = {}\n",
        "        self.encoders = {}\n",
        "\n",
        "    def _setup_logging(self) -> logging.Logger:\n",
        "        \"\"\"Setup comprehensive logging\"\"\"\n",
        "        log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        "        logging.basicConfig(\n",
        "            level=self.config.get('log_level', logging.INFO),\n",
        "            format=log_format,\n",
        "            handlers=[\n",
        "                logging.FileHandler(self.config.get('log_file', '/content/drive/My Drive/vulnerability_detection.log')),\n",
        "                logging.StreamHandler()\n",
        "            ]\n",
        "        )\n",
        "        return logging.getLogger(__name__)\n",
        "\n",
        "    def run_full_pipeline(self, log_files: List[str], app_log_files: Optional[List[str]] = None):\n",
        "        \"\"\"Run the complete vulnerability detection pipeline\"\"\"\n",
        "        try:\n",
        "            self.logger.info(\"Starting vulnerability detection pipeline...\")\n",
        "            access_logs = self.process_logs(log_files)\n",
        "\n",
        "            access_logs = self.detect_attacks(access_logs)\n",
        "\n",
        "            component_stats = self.calculate_vulnerability_scores(access_logs)\n",
        "\n",
        "            if app_log_files:\n",
        "                app_logs = self.parse_application_logs(app_log_files)\n",
        "                correlations, component_errors = self.correlate_logs(access_logs, app_logs)\n",
        "                component_stats = self.enhance_with_app_correlations(component_stats, correlations)\n",
        "\n",
        "            if self.config.get('train_models', True):\n",
        "                model_package, predictions = self.train_models(component_stats)\n",
        "            else:\n",
        "                self.load_models()\n",
        "                predictions = self.predict_vulnerabilities(component_stats)\n",
        "\n",
        "            self.generate_reports(predictions)\n",
        "            self.create_visualizations(access_logs, predictions, model_package.get('model_results', {}))\n",
        "\n",
        "            return predictions\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Pipeline failed: {e}\")\n",
        "            raise\n",
        "\n",
        "    def process_logs(self, log_files: List[str]) -> pd.DataFrame:\n",
        "        return process_logs_distributed(log_files, sample_size=self.config.get('sample_size', None))\n",
        "\n",
        "    def detect_attacks(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        return detect_attack_patterns_enhanced(df)\n",
        "\n",
        "    def calculate_vulnerability_scores(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        weights = self.config.get('vulnerability_weights', None)\n",
        "        return calculate_component_vulnerability_scores_configurable(df, weights)\n",
        "\n",
        "    def parse_application_logs(self, app_log_files: List[str]) -> pd.DataFrame:\n",
        "        return parse_application_logs(app_log_files)\n",
        "\n",
        "    def correlate_logs(self, access_logs: pd.DataFrame, app_logs: pd.DataFrame):\n",
        "        return correlate_access_and_app_logs(access_logs, app_logs)\n",
        "\n",
        "    def enhance_with_app_correlations(self, component_stats: pd.DataFrame, correlations: pd.DataFrame) -> pd.DataFrame:\n",
        "        if not correlations.empty:\n",
        "            component_stats = component_stats.merge(\n",
        "                correlations.groupby('component_name').agg({'error_count': 'sum', 'attack_induced_errors': 'sum'}),\n",
        "                on='component_name', how='left'\n",
        "            ).fillna(0)\n",
        "        return component_stats\n",
        "\n",
        "    def train_models(self, component_stats: pd.DataFrame):\n",
        "        return train_ensemble_vulnerability_model(component_stats, use_synthetic_labels=self.config.get('use_synthetic_labels', False))\n",
        "\n",
        "    def load_models(self):\n",
        "        model_path = self.config.get('model_path', '/content/drive/My Drive/ensemble_vulnerability_model.pkl')\n",
        "        self.models = joblib.load(model_path)\n",
        "        self.logger.info(f\"Loaded models from {model_path}\")\n",
        "\n",
        "    def predict_vulnerabilities(self, component_stats: pd.DataFrame) -> pd.DataFrame:\n",
        "        feature_columns = self.models['feature_columns']\n",
        "        X = component_stats[feature_columns[:-1]].fillna(0)\n",
        "        component_type_encoded = self.models['label_encoder'].transform(component_stats['component_type'])\n",
        "        X_with_type = np.column_stack([X.values, component_type_encoded])\n",
        "        X_scaled = self.models['scaler'].transform(X_with_type)\n",
        "\n",
        "        vulnerability_probs = self.models['ensemble_model'].predict_proba(X_scaled)[:, 1]\n",
        "        component_stats['vulnerability_probability'] = vulnerability_probs\n",
        "\n",
        "        if self.models.get('oc_svm'):\n",
        "            anomaly_scores = self.models['oc_svm'].decision_function(X_scaled)\n",
        "            component_stats['anomaly_score'] = -anomaly_scores\n",
        "\n",
        "        threshold = self.config.get('vulnerability_threshold', 0.5)\n",
        "        component_stats['is_vulnerable'] = component_stats['vulnerability_probability'] > threshold\n",
        "        component_stats['recommendations'] = component_stats.apply(self._generate_recommendations, axis=1)\n",
        "\n",
        "        return component_stats\n",
        "\n",
        "    def _generate_recommendations(self, component: pd.Series) -> List[str]:\n",
        "        recommendations = []\n",
        "        if component['is_vulnerable']:\n",
        "            if component['component_type'] == 'authentication':\n",
        "                recommendations.extend(['Implement rate limiting', 'Enable MFA', 'Review session management'])\n",
        "            elif component['component_type'] == 'api':\n",
        "                recommendations.extend(['Implement API rate limiting', 'Add input validation', 'Enable API authentication'])\n",
        "            elif component['component_type'] == 'file_handler':\n",
        "                recommendations.extend(['File type validation', 'Use sandboxed environments', 'Enable antivirus scanning'])\n",
        "\n",
        "            if component.get('attack_sql_injection', False):\n",
        "                recommendations.extend(['Use parameterized queries', 'Implement input validation'])\n",
        "            if component.get('attack_xss', False):\n",
        "                recommendations.extend(['Implement CSP', 'Enable output encoding'])\n",
        "            if component.get('server_error_rate', 0) > 0.1:\n",
        "                recommendations.extend(['Review error handling', 'Implement exception handling'])\n",
        "            if component['risk_score'] > 100:\n",
        "                recommendations.extend(['Conduct immediate security review', 'Consider temporary disabling'])\n",
        "\n",
        "        return recommendations\n",
        "\n",
        "    def generate_reports(self, predictions: pd.DataFrame):\n",
        "        executive_summary = self._generate_executive_summary(predictions)\n",
        "        component_report = self._generate_component_report(predictions)\n",
        "\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        with open(f'/content/drive/My Drive/vulnerability_report_{timestamp}.md', 'w') as f:\n",
        "            f.write(executive_summary + '\\n\\n' + component_report)\n",
        "\n",
        "        report_data = {\n",
        "            'timestamp': timestamp,\n",
        "            'summary': {\n",
        "                'total_components': len(predictions),\n",
        "                'vulnerable_components': predictions['is_vulnerable'].sum(),\n",
        "                'critical_components': len(predictions[predictions['risk_score'] > 200]),\n",
        "                'average_risk_score': predictions['risk_score'].mean()\n",
        "            },\n",
        "            'vulnerable_components': predictions[predictions['is_vulnerable']].to_dict('records')\n",
        "        }\n",
        "\n",
        "        with open(f'/content/drive/My Drive/vulnerability_report_{timestamp}.json', 'w') as f:\n",
        "            json.dump(report_data, f, indent=2, default=str)\n",
        "\n",
        "        self.logger.info(f\"Reports generated: vulnerability_report_{timestamp}.[md|json]\")\n",
        "\n",
        "    def _generate_executive_summary(self, predictions: pd.DataFrame) -> str:\n",
        "        vulnerable_count = predictions['is_vulnerable'].sum()\n",
        "        total_count = len(predictions)\n",
        "        vulnerability_rate = (vulnerable_count / total_count) * 100\n",
        "\n",
        "        summary = f\"\"\"# Vulnerability Detection Report - Executive Summary\n",
        "## Overview\n",
        "- Total Components Analyzed: {total_count}\n",
        "- Vulnerable Components: {vulnerable_count} ({vulnerability_rate:.1f}%)\n",
        "- Critical Risk Components: {len(predictions[predictions['risk_score'] > 200])}\n",
        "- Average Risk Score: {predictions['risk_score'].mean():.2f}\n",
        "\n",
        "## Key Findings\n",
        "1. **Most Vulnerable Component Types**:\n",
        "\"\"\"\n",
        "        vuln_by_type = predictions[predictions['is_vulnerable']].groupby('component_type').size()\n",
        "        for comp_type, count in vuln_by_type.nlargest(3).items():\n",
        "            summary += f\"   - {comp_type}: {count} vulnerable components\\n\"\n",
        "\n",
        "        summary += \"2. **Top Attack Patterns**:\\n\"\n",
        "        attack_columns = [col for col in predictions.columns if col.startswith('attack_') and col.endswith('_sum')]\n",
        "        if attack_columns:\n",
        "            attack_sums = predictions[attack_columns].sum().sort_values(ascending=False)\n",
        "            for attack, count in attack_sums.head(3).items():\n",
        "                attack_name = attack.replace('attack_', '').replace('_sum', '')\n",
        "                summary += f\"   - {attack_name}: {count} instances\\n\"\n",
        "\n",
        "        summary += f\"\"\"\n",
        "3. **Immediate Actions**:\n",
        "   - Review {len(predictions[predictions['risk_score'] > 200])} critical components\n",
        "   - Implement security measures for vulnerable components\n",
        "   - Schedule security audits\n",
        "\"\"\"\n",
        "        return summary\n",
        "\n",
        "    def _generate_component_report(self, predictions: pd.DataFrame) -> str:\n",
        "        report = \"# Detailed Component Vulnerability Report\\n\\n\"\n",
        "        vulnerable_components = predictions[predictions['is_vulnerable']].sort_values('risk_score', ascending=False)\n",
        "\n",
        "        for idx, component in vulnerable_components.head(20).iterrows():\n",
        "            report += f\"\"\"## {component['component_name']} ({component['component_type']})\n",
        "**Risk Score**: {component['risk_score']:.2f}\n",
        "**Vulnerability Probability**: {component['vulnerability_probability']:.3f}\n",
        "**Attack Rate**: {component.get('attack_rate', 0):.3f}\n",
        "**Error Rate**: {component.get('error_rate', 0):.3f}\n",
        "\n",
        "### Detected Attacks:\n",
        "\"\"\"\n",
        "            attack_columns = [col for col in component.index if col.startswith('attack_') and component[col] and not col.endswith('_score') and not col.endswith('_matches')]\n",
        "            for attack in attack_columns:\n",
        "                attack_name = attack.replace('attack_', '').replace('_', ' ').title()\n",
        "                report += f\"- {attack_name}\\n\"\n",
        "\n",
        "            report += \"\\n### Recommendations:\\n\"\n",
        "            if 'recommendations' in component and component['recommendations']:\n",
        "                for rec in component['recommendations']:\n",
        "                    report += f\"- {rec}\\n\"\n",
        "            report += \"\\n---\\n\\n\"\n",
        "\n",
        "        return report\n",
        "\n",
        "    def create_visualizations(self, access_logs: pd.DataFrame, component_stats: pd.DataFrame, model_results: Dict):\n",
        "        full_correlation_df, _, _ = analyze_attack_component_correlation_enhanced(access_logs, component_stats)\n",
        "        generate_comprehensive_visualizations(access_logs, component_stats, full_correlation_df, model_results)\n",
        "\n",
        "# Configuration\n",
        "config = {\n",
        "    'log_level': logging.INFO,\n",
        "    'log_file': '/content/drive/My Drive/vulnerability_detection.log',\n",
        "    'sample_size': None,\n",
        "    'train_models': True,\n",
        "    'use_synthetic_labels': False,\n",
        "    'vulnerability_threshold': 0.5,\n",
        "    'vulnerability_weights': {\n",
        "        'attack_rate': 100.0, 'error_rate': 50.0, 'server_errors': 2.0, 'auth_errors': 3.0,\n",
        "        'attack_severity_critical': 5.0, 'attack_severity_high': 3.0, 'attack_severity_medium': 1.0,\n",
        "        'attack_confidence': 50.0, 'unique_attack_types': 10.0, 'failed_auth_rate': 75.0\n",
        "    }\n",
        "}\n",
        "\n",
        "# Run pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    pipeline = VulnerabilityDetectionPipeline(config)\n",
        "    log_files = glob.glob('/content/drive/My Drive/logs/*.log')\n",
        "    app_log_files = glob.glob('/content/drive/My Drive/app_logs/*.log')\n",
        "    results = pipeline.run_full_pipeline(log_files, app_log_files)\n",
        "    print(f\"âœ… Pipeline completed. Found {results['is_vulnerable'].sum()} vulnerable components.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "MsL46CiN8dcp",
        "outputId": "f36b20ce-dc91-4946-fe43-6ff5f44b7e82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Pipeline failed: 'url'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'url'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-3bb3eb58ba40>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mlog_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/logs/*.log'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0mapp_log_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/app_logs/*.log'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_full_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapp_log_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"âœ… Pipeline completed. Found {results['is_vulnerable'].sum()} vulnerable components.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-3bb3eb58ba40>\u001b[0m in \u001b[0;36mrun_full_pipeline\u001b[0;34m(self, log_files, app_log_files)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0maccess_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0maccess_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_attacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccess_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mcomponent_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_vulnerability_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccess_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-3bb3eb58ba40>\u001b[0m in \u001b[0;36mdetect_attacks\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdetect_attacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdetect_attack_patterns_enhanced\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcalculate_vulnerability_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-c0a32d626567>\u001b[0m in \u001b[0;36mdetect_attack_patterns_enhanced\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ðŸŽ¯ Enhanced attack pattern detection...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'decoded_url'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode_payload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     attack_signatures = {\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/range.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    415\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHashable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_indexing_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'url'"
          ]
        }
      ]
    }
  ]
}