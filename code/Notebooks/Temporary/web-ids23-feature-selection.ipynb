{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0ca7bea",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "## Dataset: WEB-IDS23 Dataset\n",
    "#### Tasks:\n",
    "    1. Feature selection using Forward - Backward Analysis\n",
    "\n",
    "#### Navigation:\n",
    "1. [Import Libraries](#import-libraries)\n",
    "2. [Utility Functions](#utility-functions)\n",
    "3. [Load the Dataset](#load-the-dataset)\n",
    "4. [Data Preprocessing](#data-preprocessing)\n",
    "5. [Model Building and Feature Selection](#model-building-and-feature-selection)\n",
    "\n",
    "        Author: Nithusikan T.\n",
    "        Email: e19266@eng.pdn.ac.lk\n",
    "        Date: 29/05/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5800f406",
   "metadata": {},
   "source": [
    "### Import Libraries [üè†](#feature-selection) <a id=\"import-libraries\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a904234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76b59a8",
   "metadata": {},
   "source": [
    "### Utility Functions [üè†](#feature-selection) <a id=\"utility-functions\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "390a9958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_top_rows_from_csvs(folder_path, n_rows=10000):\n",
    "    \"\"\"\n",
    "    Reads the first `n_rows` from each CSV in the specified folder\n",
    "    and concatenates them into a single DataFrame.\n",
    "\n",
    "    Note: It omits the \"web-ids23_smtp_enum.csv\" file as it contains only 7 rows.\n",
    "    \n",
    "    Parameters:\n",
    "        folder_path (str): Path to the folder containing CSV files.\n",
    "        n_rows (int): Number of rows to read from each file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame from all CSVs.\n",
    "    \"\"\"\n",
    "    all_dfs = []\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".csv\") and filename not in [\"web-ids23_smtp_enum.csv\", \"ssh_login.csv\", \"ssh_login_successful.csv\"]:\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, nrows=n_rows)\n",
    "                all_dfs.append(df)\n",
    "                print(f\"{filename} loaded with {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not read {filename}: {e}\")\n",
    "    \n",
    "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "def preprocess_data(df, columns_2_drop, target_columns=['attack_type', 'attack']):\n",
    "    \"\"\"\n",
    "    Preprocess the DataFrame by handling missing values, encoding categorical variables,\n",
    "    and scaling numerical features.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input DataFrame.\n",
    "    target_column (str): Name of the target variable column.\n",
    "    columns_2_drop (list): List of columns to drop from the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    X (pd.DataFrame): Features DataFrame.\n",
    "    y_1 (pd.Series): Target variable Series.\n",
    "    y_2 (pd.Series): Second target variable Series.\n",
    "    \"\"\"\n",
    "    # Label encoder\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    # Handle missing values\n",
    "    df = df.dropna(axis=0, how='any')  # Drop rows with any missing values\n",
    "\n",
    "    # Separate features and target variable\n",
    "    y_1 = df[target_columns[0]]  # attack_type\n",
    "    y_2 = df[target_columns[1]]  # attack\n",
    "    X = df.drop(columns=target_columns)\n",
    "\n",
    "    # Drop some columns that are not needed for analysis\n",
    "    print(f\"Dropping columns: {columns_2_drop}\") # df.columns[0:4])\n",
    "    X = X.drop(columns=columns_2_drop)\n",
    "\n",
    "    # Encode categorical variables if any\n",
    "    X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "    # Replace labels with numerical values\n",
    "    if y_1.dtype == 'object':\n",
    "        y_1 = label_encoder.fit_transform(y_1)\n",
    "    \n",
    "    return pd.DataFrame(X, columns=X.columns), pd.Series(y_1), pd.Series(y_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f69cce",
   "metadata": {},
   "source": [
    "### Load the dataset [üè†](#feature-selection) <a id=\"load-the-dataset\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ec5f7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "web-ids23_benign.csv loaded with 10000 rows and 38 columns.\n",
      "web-ids23_bruteforce_http.csv loaded with 10000 rows and 38 columns.\n",
      "web-ids23_bruteforce_https.csv loaded with 10000 rows and 38 columns.\n",
      "web-ids23_dos_http.csv loaded with 10000 rows and 38 columns.\n",
      "web-ids23_dos_https.csv loaded with 10000 rows and 38 columns.\n",
      "web-ids23_ftp_login.csv loaded with 10000 rows and 38 columns.\n",
      "web-ids23_ftp_version.csv loaded with 10000 rows and 38 columns.\n",
      "web-ids23_hostsweep_Pn.csv loaded with 10000 rows and 38 columns.\n",
      "web-ids23_hostsweep_sn.csv loaded with 10000 rows and 38 columns.\n",
      "web-ids23_portscan.csv loaded with 10000 rows and 38 columns.\n",
      "web-ids23_revshell_http.csv loaded with 8549 rows and 38 columns.\n",
      "web-ids23_revshell_https.csv loaded with 9404 rows and 38 columns.\n",
      "web-ids23_smtp_version.csv loaded with 10000 rows and 38 columns.\n",
      "web-ids23_sql_injection_http.csv loaded with 10000 rows and 38 columns.\n",
      "web-ids23_sql_injection_https.csv loaded with 10000 rows and 38 columns.\n",
      "web-ids23_ssh_login.csv loaded with 10000 rows and 38 columns.\n",
      "web-ids23_ssh_login_successful.csv loaded with 10000 rows and 38 columns.\n",
      "web-ids23_ssrf_http.csv loaded with 5509 rows and 38 columns.\n",
      "web-ids23_ssrf_https.csv loaded with 6656 rows and 38 columns.\n",
      "web-ids23_xss_http.csv loaded with 4558 rows and 38 columns.\n",
      "web-ids23_xss_https.csv loaded with 4533 rows and 38 columns.\n"
     ]
    }
   ],
   "source": [
    "df = load_top_rows_from_csvs(\"E:\\\\Accadamics\\\\Semesters\\\\Final Year Project\\\\e19-4yp-The-Compound-Prediction-Analysis-of-Cybersecurity\\\\Data\\\\web-ids23\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59c62aa",
   "metadata": {},
   "source": [
    "### Data Preprocessing [üè†](#feature-selection) <a id=\"data-preprocessing\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20641b91",
   "metadata": {},
   "source": [
    "#### Drop unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dd1d1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping columns: ['uid', 'ts', 'id.orig_h', 'id.resp_h', 'service', 'traffic_direction', 'fwd_init_window_size', 'bwd_init_window_size', 'fwd_last_window_size', 'bwd_last_window_size']\n"
     ]
    }
   ],
   "source": [
    "# Drop columns that are not needed for analysis\n",
    "columns_2_drop = [\n",
    "    'uid',               # Unique flow ID (not predictive)\n",
    "    'ts',                # Timestamp (not useful directly; time-series analysis might use it differently)\n",
    "    'id.orig_h',         # Origin IP ‚Äî environment-specific\n",
    "    'id.resp_h',         # Destination IP ‚Äî environment-specific\n",
    "    'service',           # Tool-specific, may not generalize\n",
    "    'traffic_direction', # Typically derived from IPs ‚Äî not generalizable\n",
    "    'fwd_init_window_size', # Typically the window size varies with OS\n",
    "    'bwd_init_window_size', # \"\"\n",
    "    'fwd_last_window_size', # \"\"\n",
    "    'bwd_last_window_size'  # \"\"\n",
    "]\n",
    "\n",
    "target_columns = [       # Define target columns\n",
    "    'attack_type', \n",
    "    'attack'\n",
    "]  \n",
    "\n",
    "X, y_attack_type, y_attack = preprocess_data(df, columns_2_drop=columns_2_drop, target_columns=target_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e1eb0d",
   "metadata": {},
   "source": [
    "#### Feature and Label Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbc616f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "(137024, 26)\n"
     ]
    }
   ],
   "source": [
    "feature_cols = [\n",
    "    \"flow_duration\", \"fwd_pkts_tot\", \"bwd_pkts_tot\",\n",
    "    \"fwd_data_pkts_tot\", \"bwd_data_pkts_tot\",\n",
    "    \"fwd_pkts_per_sec\", \"bwd_pkts_per_sec\", \"flow_pkts_per_sec\",\n",
    "    \"down_up_ratio\", \"payload_bytes_per_second\",\n",
    "    \"flow_FIN_flag_count\", \"flow_SYN_flag_count\", \"flow_RST_flag_count\",\n",
    "    \"flow_ACK_flag_count\", \"fwd_PSH_flag_count\", \"bwd_PSH_flag_count\",\n",
    "    \"flow_CWR_flag_count\", \"flow_ECE_flag_count\",\n",
    "    \"fwd_URG_flag_count\", \"bwd_URG_flag_count\",\n",
    "    \"fwd_header_size_tot\", \"bwd_header_size_tot\",\n",
    "    \"fwd_header_size_min\", \"fwd_header_size_max\",\n",
    "    \"bwd_header_size_min\", \"bwd_header_size_max\"\n",
    "]\n",
    "\n",
    "\n",
    "print(f\"The length of feature_cols is {len(feature_cols)}.\")\n",
    "print(f\"The shape of X is {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53ffcb3",
   "metadata": {},
   "source": [
    "#### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdc7937a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X is (137024, 26)\n"
     ]
    }
   ],
   "source": [
    "X = pd.get_dummies(X, drop_first=True)\n",
    "print(f\"The shape of X is {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a42b9e",
   "metadata": {},
   "source": [
    "#### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc9f6f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_attack_type, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15501ec6",
   "metadata": {},
   "source": [
    "### Model Building and Feature Selection [üè†](#feature-selection) <a id=\"model-building-and-feature-selection\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cda4ece8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "rfc = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f88796",
   "metadata": {},
   "source": [
    "#### Forward feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19de2f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs_forward = SFS(\n",
    "    rfc,\n",
    "    k_features='best',\n",
    "    forward=True,\n",
    "    floating=False,\n",
    "    scoring='accuracy',\n",
    "    cv=5,\n",
    "    n_jobs=-1 \n",
    ")\n",
    "\n",
    "sfs_forward = sfs_forward.fit(X_train, y_train)\n",
    "\n",
    "print(\"üîç Forward Selection Best Features:\")\n",
    "print(list(sfs_forward.k_feature_names_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4331aeeb",
   "metadata": {},
   "source": [
    "#### Backward feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905e17b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs_backward = SFS(rfc,\n",
    "                   k_features='best',\n",
    "                   forward=False,\n",
    "                   floating=False,\n",
    "                   scoring='accuracy',\n",
    "                   cv=5,\n",
    "                   n_jobs=-1)\n",
    "\n",
    "sfs_backward = sfs_backward.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nüîç Backward Elimination Best Features:\")\n",
    "print(list(sfs_backward.k_feature_names_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8cf97e",
   "metadata": {},
   "source": [
    "#### Selected Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0902468",
   "metadata": {},
   "outputs": [],
   "source": [
    "union_features = list(sfs_backward.k_feature_names_) | list(sfs_backward.k_feature_names_)\n",
    "print(\"The union of fwd and bwd features:\\n\")\n",
    "print(union_features)\n",
    "\n",
    "intersection_features = list(sfs_backward.k_feature_names_) & list(sfs_backward.k_feature_names_)\n",
    "print(\"\\nThe common (intersection) of fwd and bwd features:\\n\")\n",
    "print(intersection_features)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web_ids_23_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
